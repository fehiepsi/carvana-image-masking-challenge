{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'train_path': '../data/train4320/',\n",
    "    'train_masks_path': '../data/train4320_masks/',\n",
    "    'val_path': '../data/val768/',\n",
    "    'val_masks_path': '../data/val768_masks/',\n",
    "    'split_data': False,\n",
    "    'batch_size': 3,\n",
    "    'log_every': 10,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'test': False,\n",
    "    'seed': 20170915,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join('../intermediate/', str(args.seed), 't')\n",
    "if not os.path.isdir(args.intermediate_path):\n",
    "    os.mkdir(args.intermediate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.split_data:\n",
    "    # !mogrify -format png *.gif\n",
    "    if not os.path.isdir(args.train_path):\n",
    "        os.mkdir(args.train_path)\n",
    "    if not os.path.isdir(args.train_masks_path):\n",
    "        os.mkdir(args.train_masks_path)\n",
    "    if not os.path.isdir(args.val_path):\n",
    "        os.mkdir(args.val_path)\n",
    "    if not os.path.isdir(args.val_masks_path):\n",
    "        os.mkdir(args.val_masks_path)\n",
    "    files = sorted([x.split('/')[-1] for x in glob.glob('../data/train_hq/*.jpg')])\n",
    "    random.seed(args.seed)\n",
    "    random.shuffle(files)\n",
    "    for filename in files[:4320]:\n",
    "        image = cv2.imread('../data/train_hq/' + filename)\n",
    "        image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "        cv2.imwrite(os.path.join(args.train_path, filename), image)\n",
    "        mask_filename = '../data/train_masks/' + filename.replace('.jpg', '_mask.png')\n",
    "        shutil.copy2(mask_filename, args.train_masks_path)\n",
    "    for filename in files[4320:]:\n",
    "        image = cv2.imread('../data/train_hq/' + filename)\n",
    "        image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "        cv2.imwrite(os.path.join(args.val_path, filename), image)\n",
    "        mask_filename = '../data/train_masks/' + filename.replace('.jpg', '_mask.png')\n",
    "        shutil.copy2(mask_filename, args.val_masks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnRelu2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):\n",
    "        super(ConvBnRelu2d, self).__init__()\n",
    "        padding = kernel_size//2 * dilation\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, dilation=dilation, padding=padding, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels, eps=1e-4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            o = self.bn(o)\n",
    "        return self.relu(o)\n",
    "\n",
    "    def merge_bn(self):  # for faster inference\n",
    "        if self.bn is None:\n",
    "            return\n",
    "        \n",
    "        conv_weight     = self.conv.weight.data\n",
    "        bn_weight       = self.bn.weight.data\n",
    "        bn_bias         = self.bn.bias.data\n",
    "        bn_running_mean = self.bn.running_mean\n",
    "        bn_running_var  = self.bn.running_var\n",
    "        bn_eps          = self.bn.eps\n",
    "\n",
    "        N,C,H,W = conv_weight.size()\n",
    "        std = torch.sqrt(bn_running_var+bn_eps)\n",
    "        std_bn_weight = (bn_weight/std).repeat(C*H*W,1).t().contiguous().view(N,C,H,W)\n",
    "        conv_weight_hat = std_bn_weight*conv_weight\n",
    "        conv_bias_hat   = bn_bias - (bn_weight/std)*bn_running_mean\n",
    "        \n",
    "        self.conv = nn.Conv2d(self.conv.in_channels, self.conv.out_channels, self.conv.kernel_size,\n",
    "                              padding=self.conv.padding, bias=True)\n",
    "        self.conv.weight.data = conv_weight_hat\n",
    "        self.conv.bias.data   = conv_bias_hat\n",
    "        self.bn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(StackEncoder, self).__init__()\n",
    "        self.encode = nn.Sequential(\n",
    "            ConvBnRelu2d(in_channels, out_channels, kernel_size),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size, dilation=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.encode(x)\n",
    "        o = F.max_pool2d(e, kernel_size=2, stride=2)\n",
    "        return e, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, en_channels, in_channels, out_channels, kernel_size=3):\n",
    "        super(StackDecoder, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)\n",
    "        self.decode = nn.Sequential(\n",
    "            ConvBnRelu2d(en_channels+in_channels, out_channels, kernel_size=kernel_size),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size=kernel_size),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size=kernel_size))\n",
    "\n",
    "    def forward(self, e, x):\n",
    "        N,C,H,W = e.size()\n",
    "        x = self.upsample(x)\n",
    "#        x = F.upsample(x, size=(H,W), mode='bilinear')\n",
    "        x = torch.cat([e, x], dim=1)\n",
    "        return self.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet1024(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_shape):\n",
    "        super(UNet1024, self).__init__()\n",
    "        C,H,W = in_shape\n",
    "\n",
    "        # 1024\n",
    "        self.down1 = StackEncoder(  C,  24)  # 512\n",
    "        self.down2 = StackEncoder( 24,  64)  # 256\n",
    "        self.down3 = StackEncoder( 64, 128)  # 128\n",
    "        self.down4 = StackEncoder(128, 256)  # 64\n",
    "        self.down5 = StackEncoder(256, 512)  # 32\n",
    "        self.down6 = StackEncoder(512, 768)  # 16\n",
    "\n",
    "        self.center = ConvBnRelu2d(768, 768)\n",
    "\n",
    "        # 16\n",
    "        self.up6 = StackDecoder(768, 768, 512)  # 32\n",
    "        self.up5 = StackDecoder(512, 512, 256)  # 64\n",
    "        self.up4 = StackDecoder(256, 256, 128)  # 128\n",
    "        self.up3 = StackDecoder(128, 128,  64)  # 256\n",
    "        self.up2 = StackDecoder( 64,  64,  24)  # 512\n",
    "        self.up1 = StackDecoder( 24,  24,  24)  # 1024\n",
    "        \n",
    "        self.mask = nn.Conv2d(24, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1, o = self.down1(x)\n",
    "        e2, o = self.down2(o)\n",
    "        e3, o = self.down3(o)\n",
    "        e4, o = self.down4(o)\n",
    "        e5, o = self.down5(o)\n",
    "        e6, o = self.down6(o)\n",
    "\n",
    "        o = self.center(o)\n",
    "        \n",
    "        o = self.up6(e6, o)\n",
    "        o = self.up5(e5, o)\n",
    "        o = self.up4(e4, o)\n",
    "        o = self.up3(e3, o)\n",
    "        o = self.up2(e2, o)\n",
    "        o = self.up1(e1, o)\n",
    "\n",
    "        o = self.mask(o)\n",
    "        o = F.upsample(o, size=(1280,1918), mode='bilinear')\n",
    "        return torch.squeeze(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(probs, target, weight=None, use_mask=True, threshold=0.5):\n",
    "    if use_mask:\n",
    "        probs = (probs > threshold).float()\n",
    "    N     = target.size(0)\n",
    "    if weight is None:\n",
    "        w = Variable(torch.ones(target.size()).cuda()).view(N, -1)\n",
    "    else:\n",
    "        w = weight.view(N, -1)\n",
    "    w2    = w*w\n",
    "    m1    = probs.view(N, -1)\n",
    "    m2    = target.view(N, -1)\n",
    "    score = (2*(w2*m1*m2).sum(dim=1) + 1) / ((w2*m1).sum(dim=1) + (w2*m2).sum(dim=1) + 1)\n",
    "    \n",
    "    return score.sum()/N\n",
    "\n",
    "\n",
    "def dice_loss(logits, target, weight=None):\n",
    "    probs = F.sigmoid(logits)\n",
    "    loss  = 1 - dice_score(probs, target, weight, use_mask=False)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def criterion(logits, target):\n",
    "    N,H,W = target.size()\n",
    "    a = F.avg_pool2d(target, kernel_size=41, stride=1, padding=20)\n",
    "    boundary = (a.ge(0.01) * a.le(0.99)).float()\n",
    "    weight = Variable(torch.ones(a.size()).cuda())\n",
    "    w0 = weight.sum()\n",
    "    weight = weight + 2*boundary\n",
    "    w1 = weight.sum()\n",
    "    weight = weight*w0/w1\n",
    "        \n",
    "    return (F.binary_cross_entropy_with_logits(logits, target, weight)\n",
    "            + dice_loss(logits, target, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image):\n",
    "    image = image.transpose((2,0,1)).astype(np.float32)  # HWC -> CHW\n",
    "    tensor = torch.from_numpy(image)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def label_to_tensor(label, threshold=0.5):\n",
    "    label  = (label>threshold).astype(np.float32)\n",
    "    tensor = torch.from_numpy(label)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_path, mask_path='', transform=[], mode='train'):\n",
    "        super(CarDataset, self).__init__()\n",
    "        self.img_names = sorted([x.split('/')[-1] for x in glob.glob(image_path + '/*.jpg')])\n",
    "        self.img_path  = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.transform = transform\n",
    "        self.mode      = mode\n",
    "\n",
    "    def get_image(self, index):\n",
    "        name  = self.img_names[index]\n",
    "        file  = os.path.join(self.img_path, name)\n",
    "        img   = cv2.imread(file)\n",
    "        image = img / 255\n",
    "        return image, name\n",
    "    \n",
    "    def get_label(self, name):\n",
    "        name = name.replace('.jpg', '_mask.png')\n",
    "        file = os.path.join(self.mask_path, name)\n",
    "        mask = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        label = mask / 255\n",
    "        return label\n",
    "\n",
    "    def get_train_item(self, index):\n",
    "        image, name = self.get_image(index)\n",
    "        label = self.get_label(name)\n",
    "        image = image_to_tensor(image)\n",
    "        label = label_to_tensor(label)\n",
    "        return image, label\n",
    "\n",
    "    def get_test_item(self, index):\n",
    "        image = self.get_image(index)\n",
    "        image = image_to_tensor(image)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            return self.get_train_item(index)\n",
    "        elif self.mode == 'test':\n",
    "            return self.get_test_item(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "    num_grad_acc = 16 // args.batch_size\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    init_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    for i, (inputt, target) in enumerate(train_loader, 1):\n",
    "        inputt = inputt.cuda()\n",
    "        target = target.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = model(inputt)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        if i % num_grad_acc == 0:\n",
    "            nn.utils.clip_grad_norm(model.parameters(), 3, 'inf')\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | Train loss: {:.4f}\"\n",
    "                  .format(time.time()-init_time, i, loss.data[0]))\n",
    "    return train_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model):\n",
    "    model.eval()\n",
    "    val_score = 0\n",
    "    for i, (inputt, target) in enumerate(val_loader, 1):\n",
    "        inputt = inputt.cuda()\n",
    "        target = target.cuda()\n",
    "        inputt = Variable(inputt, volatile=True)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = model(inputt)\n",
    "        score = dice_score(F.sigmoid(output), target)\n",
    "        val_score += score.data[0] * inputt.size(0)\n",
    "    return val_score / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, score):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_{}_epoch{}_score{:.5f}.pth\"\n",
    "                              .format(args.seed, epoch, score))\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CarDataset(args.train_path, args.train_masks_path,\n",
    "                           transform=[], mode='train')\n",
    "train_loader  = DataLoader(train_dataset, args.batch_size, shuffle=True, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CarDataset(args.val_path, args.val_masks_path,\n",
    "                         transform=[], mode='train')\n",
    "val_loader  = DataLoader(val_dataset, args.batch_size, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet1024((3, 1024, 1024))\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1 with lr [0.01]\n",
      "   % Time:   16s | Batch:   10 | Train loss: 1.3858\n",
      "   % Time:   23s | Batch:   20 | Train loss: 1.1657\n",
      "   % Time:   30s | Batch:   30 | Train loss: 1.2590\n",
      "   % Time:   38s | Batch:   40 | Train loss: 1.0610\n",
      "   % Time:   45s | Batch:   50 | Train loss: 0.9662\n",
      "   % Time:   52s | Batch:   60 | Train loss: 0.8081\n",
      "   % Time:   60s | Batch:   70 | Train loss: 0.9162\n",
      "   % Time:   67s | Batch:   80 | Train loss: 0.7613\n",
      "   % Time:   74s | Batch:   90 | Train loss: 0.6056\n",
      "   % Time:   82s | Batch:  100 | Train loss: 0.5777\n",
      "   % Time:   89s | Batch:  110 | Train loss: 0.5928\n",
      "   % Time:   96s | Batch:  120 | Train loss: 0.5393\n",
      "   % Time:  104s | Batch:  130 | Train loss: 0.9927\n",
      "   % Time:  111s | Batch:  140 | Train loss: 0.5253\n",
      "   % Time:  118s | Batch:  150 | Train loss: 0.4374\n",
      "   % Time:  126s | Batch:  160 | Train loss: 0.5917\n",
      "   % Time:  133s | Batch:  170 | Train loss: 0.3983\n",
      "   % Time:  141s | Batch:  180 | Train loss: 0.4014\n",
      "   % Time:  148s | Batch:  190 | Train loss: 0.3648\n",
      "   % Time:  155s | Batch:  200 | Train loss: 0.3366\n",
      "   % Time:  163s | Batch:  210 | Train loss: 0.3606\n",
      "   % Time:  170s | Batch:  220 | Train loss: 0.3267\n",
      "   % Time:  177s | Batch:  230 | Train loss: 0.4980\n",
      "   % Time:  185s | Batch:  240 | Train loss: 0.3881\n",
      "   % Time:  192s | Batch:  250 | Train loss: 0.3715\n",
      "   % Time:  200s | Batch:  260 | Train loss: 0.4837\n",
      "   % Time:  207s | Batch:  270 | Train loss: 0.3010\n",
      "   % Time:  214s | Batch:  280 | Train loss: 0.3801\n",
      "   % Time:  222s | Batch:  290 | Train loss: 0.3459\n",
      "   % Time:  229s | Batch:  300 | Train loss: 0.2622\n",
      "   % Time:  236s | Batch:  310 | Train loss: 0.2844\n",
      "   % Time:  244s | Batch:  320 | Train loss: 0.2826\n",
      "   % Time:  251s | Batch:  330 | Train loss: 0.3402\n",
      "   % Time:  259s | Batch:  340 | Train loss: 0.2302\n",
      "   % Time:  266s | Batch:  350 | Train loss: 0.3755\n",
      "   % Time:  273s | Batch:  360 | Train loss: 0.1983\n",
      "   % Time:  281s | Batch:  370 | Train loss: 0.2404\n",
      "   % Time:  288s | Batch:  380 | Train loss: 0.2313\n",
      "   % Time:  295s | Batch:  390 | Train loss: 0.2907\n",
      "   % Time:  303s | Batch:  400 | Train loss: 0.2948\n",
      "   % Time:  310s | Batch:  410 | Train loss: 0.2252\n",
      "   % Time:  318s | Batch:  420 | Train loss: 0.2509\n",
      "   % Time:  325s | Batch:  430 | Train loss: 0.2092\n",
      "   % Time:  332s | Batch:  440 | Train loss: 0.1858\n",
      "   % Time:  340s | Batch:  450 | Train loss: 0.1643\n",
      "   % Time:  347s | Batch:  460 | Train loss: 0.1546\n",
      "   % Time:  354s | Batch:  470 | Train loss: 0.2097\n",
      "   % Time:  362s | Batch:  480 | Train loss: 0.2108\n",
      "   % Time:  369s | Batch:  490 | Train loss: 0.2318\n",
      "   % Time:  377s | Batch:  500 | Train loss: 0.1664\n",
      "   % Time:  384s | Batch:  510 | Train loss: 0.1607\n",
      "   % Time:  391s | Batch:  520 | Train loss: 0.1993\n",
      "   % Time:  399s | Batch:  530 | Train loss: 0.1627\n",
      "   % Time:  406s | Batch:  540 | Train loss: 0.1554\n",
      "   % Time:  413s | Batch:  550 | Train loss: 0.2430\n",
      "   % Time:  421s | Batch:  560 | Train loss: 0.1908\n",
      "   % Time:  428s | Batch:  570 | Train loss: 0.1988\n",
      "   % Time:  436s | Batch:  580 | Train loss: 0.1815\n",
      "   % Time:  443s | Batch:  590 | Train loss: 0.1954\n",
      "   % Time:  450s | Batch:  600 | Train loss: 0.1755\n",
      "   % Time:  458s | Batch:  610 | Train loss: 0.3756\n",
      "   % Time:  465s | Batch:  620 | Train loss: 0.1673\n",
      "   % Time:  473s | Batch:  630 | Train loss: 0.1103\n",
      "   % Time:  480s | Batch:  640 | Train loss: 0.1577\n",
      "   % Time:  487s | Batch:  650 | Train loss: 0.1994\n",
      "   % Time:  495s | Batch:  660 | Train loss: 0.2273\n",
      "   % Time:  502s | Batch:  670 | Train loss: 0.1097\n",
      "   % Time:  509s | Batch:  680 | Train loss: 0.1241\n",
      "   % Time:  517s | Batch:  690 | Train loss: 0.0957\n",
      "   % Time:  524s | Batch:  700 | Train loss: 0.2434\n",
      "   % Time:  532s | Batch:  710 | Train loss: 0.0975\n",
      "   % Time:  539s | Batch:  720 | Train loss: 0.1033\n",
      "   % Time:  546s | Batch:  730 | Train loss: 0.1129\n",
      "   % Time:  554s | Batch:  740 | Train loss: 0.1953\n",
      "   % Time:  561s | Batch:  750 | Train loss: 0.1190\n",
      "   % Time:  569s | Batch:  760 | Train loss: 0.1199\n",
      "   % Time:  576s | Batch:  770 | Train loss: 0.1361\n",
      "   % Time:  583s | Batch:  780 | Train loss: 0.0943\n",
      "   % Time:  591s | Batch:  790 | Train loss: 0.0913\n",
      "   % Time:  598s | Batch:  800 | Train loss: 0.0952\n",
      "   % Time:  605s | Batch:  810 | Train loss: 0.1195\n",
      "   % Time:  613s | Batch:  820 | Train loss: 0.1316\n",
      "   % Time:  620s | Batch:  830 | Train loss: 0.0961\n",
      "   % Time:  628s | Batch:  840 | Train loss: 0.1139\n",
      "   % Time:  635s | Batch:  850 | Train loss: 0.1824\n",
      "   % Time:  642s | Batch:  860 | Train loss: 0.1071\n",
      "   % Time:  650s | Batch:  870 | Train loss: 0.1108\n",
      "   % Time:  657s | Batch:  880 | Train loss: 0.1237\n",
      "   % Time:  665s | Batch:  890 | Train loss: 0.0932\n",
      "   % Time:  672s | Batch:  900 | Train loss: 0.1220\n",
      "   % Time:  679s | Batch:  910 | Train loss: 0.1219\n",
      "   % Time:  687s | Batch:  920 | Train loss: 0.1057\n",
      "   % Time:  694s | Batch:  930 | Train loss: 0.1620\n",
      "   % Time:  701s | Batch:  940 | Train loss: 0.1018\n",
      "   % Time:  709s | Batch:  950 | Train loss: 0.1090\n",
      "   % Time:  716s | Batch:  960 | Train loss: 0.0970\n",
      "   % Time:  724s | Batch:  970 | Train loss: 0.1466\n",
      "   % Time:  731s | Batch:  980 | Train loss: 0.0968\n",
      "   % Time:  738s | Batch:  990 | Train loss: 0.0850\n",
      "   % Time:  746s | Batch: 1000 | Train loss: 0.0935\n",
      "   % Time:  753s | Batch: 1010 | Train loss: 0.0767\n",
      "   % Time:  761s | Batch: 1020 | Train loss: 0.0967\n",
      "   % Time:  768s | Batch: 1030 | Train loss: 0.1265\n",
      "   % Time:  775s | Batch: 1040 | Train loss: 0.1327\n",
      "   % Time:  783s | Batch: 1050 | Train loss: 0.0818\n",
      "   % Time:  790s | Batch: 1060 | Train loss: 0.1163\n",
      "   % Time:  797s | Batch: 1070 | Train loss: 0.1027\n",
      "   % Time:  805s | Batch: 1080 | Train loss: 0.1258\n",
      "   % Time:  812s | Batch: 1090 | Train loss: 0.1225\n",
      "   % Time:  820s | Batch: 1100 | Train loss: 0.0978\n",
      "   % Time:  827s | Batch: 1110 | Train loss: 0.1029\n",
      "   % Time:  834s | Batch: 1120 | Train loss: 0.1063\n",
      "   % Time:  842s | Batch: 1130 | Train loss: 0.0840\n",
      "   % Time:  849s | Batch: 1140 | Train loss: 0.0717\n",
      "   % Time:  857s | Batch: 1150 | Train loss: 0.1078\n",
      "   % Time:  864s | Batch: 1160 | Train loss: 0.0916\n",
      "   % Time:  871s | Batch: 1170 | Train loss: 0.1317\n",
      "   % Time:  879s | Batch: 1180 | Train loss: 0.0752\n",
      "   % Time:  886s | Batch: 1190 | Train loss: 0.1136\n",
      "   % Time:  893s | Batch: 1200 | Train loss: 0.0824\n",
      "   % Time:  901s | Batch: 1210 | Train loss: 0.0781\n",
      "   % Time:  908s | Batch: 1220 | Train loss: 0.0888\n",
      "   % Time:  916s | Batch: 1230 | Train loss: 0.1008\n",
      "   % Time:  923s | Batch: 1240 | Train loss: 0.0863\n",
      "   % Time:  930s | Batch: 1250 | Train loss: 0.0929\n",
      "   % Time:  938s | Batch: 1260 | Train loss: 0.0805\n",
      "   % Time:  945s | Batch: 1270 | Train loss: 0.0799\n",
      "   % Time:  953s | Batch: 1280 | Train loss: 0.0805\n",
      "   % Time:  960s | Batch: 1290 | Train loss: 0.1032\n",
      "   % Time:  967s | Batch: 1300 | Train loss: 0.0984\n",
      "   % Time:  975s | Batch: 1310 | Train loss: 0.1134\n",
      "   % Time:  982s | Batch: 1320 | Train loss: 0.1347\n",
      "   % Time:  990s | Batch: 1330 | Train loss: 0.0902\n",
      "   % Time:  997s | Batch: 1340 | Train loss: 0.0682\n",
      "   % Time: 1004s | Batch: 1350 | Train loss: 0.0920\n",
      "   % Time: 1012s | Batch: 1360 | Train loss: 0.0994\n",
      "   % Time: 1019s | Batch: 1370 | Train loss: 0.1687\n",
      "   % Time: 1026s | Batch: 1380 | Train loss: 0.0722\n",
      "   % Time: 1034s | Batch: 1390 | Train loss: 0.0867\n",
      "   % Time: 1041s | Batch: 1400 | Train loss: 0.0772\n",
      "   % Time: 1049s | Batch: 1410 | Train loss: 0.0715\n",
      "   % Time: 1056s | Batch: 1420 | Train loss: 0.0754\n",
      "   % Time: 1063s | Batch: 1430 | Train loss: 0.0864\n",
      "   % Time: 1071s | Batch: 1440 | Train loss: 0.0910\n",
      "==========\n",
      "   % Epoch: 1 | Time: 1121s | Train loss: 0.24043 | Val score: 0.98983\n",
      "==========\n",
      "=> EPOCH 2 with lr [0.01]\n",
      "   % Time:    8s | Batch:   10 | Train loss: 0.0602\n",
      "   % Time:   15s | Batch:   20 | Train loss: 0.0711\n",
      "   % Time:   23s | Batch:   30 | Train loss: 0.0689\n",
      "   % Time:   30s | Batch:   40 | Train loss: 0.1083\n",
      "   % Time:   37s | Batch:   50 | Train loss: 0.0676\n",
      "   % Time:   45s | Batch:   60 | Train loss: 0.0895\n",
      "   % Time:   52s | Batch:   70 | Train loss: 0.0890\n",
      "   % Time:   60s | Batch:   80 | Train loss: 0.0900\n",
      "   % Time:   67s | Batch:   90 | Train loss: 0.0783\n",
      "   % Time:   74s | Batch:  100 | Train loss: 0.0743\n",
      "   % Time:   82s | Batch:  110 | Train loss: 0.0665\n",
      "   % Time:   89s | Batch:  120 | Train loss: 0.0663\n",
      "   % Time:   96s | Batch:  130 | Train loss: 0.0884\n",
      "   % Time:  104s | Batch:  140 | Train loss: 0.0844\n",
      "   % Time:  111s | Batch:  150 | Train loss: 0.1318\n",
      "   % Time:  119s | Batch:  160 | Train loss: 0.0903\n",
      "   % Time:  126s | Batch:  170 | Train loss: 0.0924\n",
      "   % Time:  133s | Batch:  180 | Train loss: 0.0752\n",
      "   % Time:  141s | Batch:  190 | Train loss: 0.1009\n",
      "   % Time:  148s | Batch:  200 | Train loss: 0.0804\n",
      "   % Time:  156s | Batch:  210 | Train loss: 0.0718\n",
      "   % Time:  163s | Batch:  220 | Train loss: 0.0875\n",
      "   % Time:  170s | Batch:  230 | Train loss: 0.0600\n",
      "   % Time:  178s | Batch:  240 | Train loss: 0.2065\n",
      "   % Time:  185s | Batch:  250 | Train loss: 0.0715\n",
      "   % Time:  193s | Batch:  260 | Train loss: 0.0645\n",
      "   % Time:  200s | Batch:  270 | Train loss: 0.0725\n",
      "   % Time:  207s | Batch:  280 | Train loss: 0.0662\n",
      "   % Time:  215s | Batch:  290 | Train loss: 0.0688\n",
      "   % Time:  222s | Batch:  300 | Train loss: 0.0851\n",
      "   % Time:  230s | Batch:  310 | Train loss: 0.1062\n",
      "   % Time:  237s | Batch:  320 | Train loss: 0.0681\n",
      "   % Time:  244s | Batch:  330 | Train loss: 0.0773\n",
      "   % Time:  252s | Batch:  340 | Train loss: 0.0705\n",
      "   % Time:  259s | Batch:  350 | Train loss: 0.1017\n",
      "   % Time:  267s | Batch:  360 | Train loss: 0.0658\n",
      "   % Time:  274s | Batch:  370 | Train loss: 0.0572\n",
      "   % Time:  281s | Batch:  380 | Train loss: 0.0569\n",
      "   % Time:  289s | Batch:  390 | Train loss: 0.0504\n",
      "   % Time:  296s | Batch:  400 | Train loss: 0.0560\n",
      "   % Time:  304s | Batch:  410 | Train loss: 0.0484\n",
      "   % Time:  311s | Batch:  420 | Train loss: 0.1142\n",
      "   % Time:  319s | Batch:  430 | Train loss: 0.0801\n",
      "   % Time:  326s | Batch:  440 | Train loss: 0.0568\n",
      "   % Time:  333s | Batch:  450 | Train loss: 0.0974\n",
      "   % Time:  341s | Batch:  460 | Train loss: 0.0602\n",
      "   % Time:  348s | Batch:  470 | Train loss: 0.0685\n",
      "   % Time:  356s | Batch:  480 | Train loss: 0.1012\n",
      "   % Time:  363s | Batch:  490 | Train loss: 0.0630\n",
      "   % Time:  370s | Batch:  500 | Train loss: 0.0559\n",
      "   % Time:  378s | Batch:  510 | Train loss: 0.0561\n",
      "   % Time:  385s | Batch:  520 | Train loss: 0.0789\n",
      "   % Time:  393s | Batch:  530 | Train loss: 0.0872\n",
      "   % Time:  400s | Batch:  540 | Train loss: 0.0622\n",
      "   % Time:  407s | Batch:  550 | Train loss: 0.0646\n",
      "   % Time:  415s | Batch:  560 | Train loss: 0.0691\n",
      "   % Time:  422s | Batch:  570 | Train loss: 0.0754\n",
      "   % Time:  430s | Batch:  580 | Train loss: 0.0771\n",
      "   % Time:  437s | Batch:  590 | Train loss: 0.0693\n",
      "   % Time:  444s | Batch:  600 | Train loss: 0.0744\n",
      "   % Time:  452s | Batch:  610 | Train loss: 0.0595\n",
      "   % Time:  459s | Batch:  620 | Train loss: 0.0567\n",
      "   % Time:  467s | Batch:  630 | Train loss: 0.0435\n",
      "   % Time:  474s | Batch:  640 | Train loss: 0.0854\n",
      "   % Time:  481s | Batch:  650 | Train loss: 0.0460\n",
      "   % Time:  489s | Batch:  660 | Train loss: 0.0624\n",
      "   % Time:  496s | Batch:  670 | Train loss: 0.0619\n",
      "   % Time:  504s | Batch:  680 | Train loss: 0.0533\n",
      "   % Time:  511s | Batch:  690 | Train loss: 0.0907\n",
      "   % Time:  518s | Batch:  700 | Train loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-18:\n",
      "Process Process-21:\n",
      "Process Process-17:\n",
      "Process Process-20:\n",
      "Process Process-19:\n",
      "Process Process-16:\n",
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7b89a7c45488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> EPOCH {} with lr {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-8125c501c7e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_grad_acc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnorm_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnorm_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    scheduler.step()\n",
    "    print(\"=> EPOCH {} with lr {}\".format(epoch, scheduler.get_lr()))\n",
    "    init_time = time.time()\n",
    "    train_loss = train(train_loader, model, optimizer)\n",
    "    val_score = validate(val_loader, model)\n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:3.0f}s | \"\n",
    "          \"Train loss: {:.5f} | Val score: {:.5f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss, val_score))\n",
    "    print(\"=\"*10)\n",
    "    save_model(model, epoch, val_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydata)",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
