{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:44.999449Z",
     "start_time": "2017-09-21T14:12:44.986206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.351928Z",
     "start_time": "2017-09-21T14:12:45.004381Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.522295Z",
     "start_time": "2017-09-21T14:12:45.353180Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.598490Z",
     "start_time": "2017-09-21T14:12:45.525691Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'train_path': '../data/train4320/',\n",
    "    'train_masks_path': '../data/train4320_masks/',\n",
    "    'val_path': '../data/val768/',\n",
    "    'val_masks_path': '../data/val768_masks/',\n",
    "    'output_path': '../intermediate/output_tn/',\n",
    "    'split_data': False,\n",
    "    'batch_size': 3,\n",
    "    'log_every': 80,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'test': False,\n",
    "    'seed': 20170921,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join('../intermediate/',\n",
    "                                      str(args.seed), 'tn')\n",
    "if not os.path.isdir(args.intermediate_path):\n",
    "    os.mkdir(args.intermediate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.667702Z",
     "start_time": "2017-09-21T14:12:45.599757Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.split_data:\n",
    "    # !mogrify -format png *.gif\n",
    "    if not os.path.isdir(args.train_path):\n",
    "        os.mkdir(args.train_path)\n",
    "    if not os.path.isdir(args.train_masks_path):\n",
    "        os.mkdir(args.train_masks_path)\n",
    "    if not os.path.isdir(args.val_path):\n",
    "        os.mkdir(args.val_path)\n",
    "    if not os.path.isdir(args.val_masks_path):\n",
    "        os.mkdir(args.val_masks_path)\n",
    "    files = sorted([x.split('/')[-1]\n",
    "                    for x in glob.glob('../data/train_hq/*.jpg')])\n",
    "    random.seed(args.seed)\n",
    "    random.shuffle(files)\n",
    "    for filename in files[:4320]:\n",
    "        image = cv2.imread('../data/train_hq/' + filename)\n",
    "        image = cv2.resize(image, (1024, 1024))\n",
    "        cv2.imwrite(os.path.join(args.train_path, filename), image)\n",
    "        mask_filename = '../data/train_masks/'+filename.replace('.jpg',\n",
    "                                                                '_mask.png')\n",
    "        shutil.copy2(mask_filename, args.train_masks_path)\n",
    "    for filename in files[4320:]:\n",
    "        image = cv2.imread('../data/train_hq/' + filename)\n",
    "        image = cv2.resize(image, (1024, 1024))\n",
    "        cv2.imwrite(os.path.join(args.val_path, filename), image)\n",
    "        mask_filename = '../data/train_masks/'+filename.replace('.jpg',\n",
    "                                                                '_mask.png')\n",
    "        shutil.copy2(mask_filename, args.val_masks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.725897Z",
     "start_time": "2017-09-21T14:12:45.669123Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvBnRelu2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):\n",
    "        super(ConvBnRelu2d, self).__init__()\n",
    "        padding = kernel_size//2 * dilation\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              dilation=dilation, padding=padding, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            o = self.bn(o)\n",
    "        return F.relu(o, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.795396Z",
     "start_time": "2017-09-21T14:12:45.730021Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(StackEncoder, self).__init__()\n",
    "        self.encode = nn.Sequential(\n",
    "            ConvBnRelu2d(in_channels, out_channels, kernel_size),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.encode(x)\n",
    "        o = F.max_pool2d(e, kernel_size=2, stride=2)\n",
    "        return e, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.848683Z",
     "start_time": "2017-09-21T14:12:45.799604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, e_channels, in_channels, out_channels, kernel_size=3):\n",
    "        super(StackDecoder, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, \\\n",
    "                                           kernel_size=2, stride=2)\n",
    "        self.decode = nn.Sequential(\n",
    "            ConvBnRelu2d(e_channels+out_channels, out_channels, kernel_size),\n",
    "#            ConvBnRelu2d(out_channels, out_channels, kernel_size=kernel_size),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size))\n",
    "\n",
    "    def forward(self, e, x):\n",
    "        N,C,H,W = e.size()\n",
    "        x = self.upsample(x)\n",
    "#        x = F.upsample(x, size=(H,W), mode='bilinear')\n",
    "        x = torch.cat([e, x], dim=1)\n",
    "        return self.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.916156Z",
     "start_time": "2017-09-21T14:12:45.850760Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UNet1024(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_shape):\n",
    "        super(UNet1024, self).__init__()\n",
    "        C,H,W = in_shape\n",
    "\n",
    "        # 1024\n",
    "        self.down1 = StackEncoder(  C,  32)  # 512\n",
    "        self.down2 = StackEncoder( 32,  64)  # 256\n",
    "        self.down3 = StackEncoder( 64, 128)  # 128\n",
    "        self.down4 = StackEncoder(128, 256)  # 64\n",
    "        self.down5 = StackEncoder(256, 512)  # 32\n",
    "#        self.down6 = StackEncoder(512, 1024)  # 16\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            ConvBnRelu2d(512, 1024),\n",
    "            ConvBnRelu2d(1024, 1024),\n",
    "        )\n",
    "\n",
    "        # 16\n",
    "#        self.up6 = StackDecoder(512, 768, 512)  # 32\n",
    "        self.up5 = StackDecoder(512, 1024, 512)  # 64\n",
    "        self.up4 = StackDecoder(256, 512, 256)  # 128\n",
    "        self.up3 = StackDecoder(128, 256, 128)  # 256\n",
    "        self.up2 = StackDecoder( 64, 128,  64)  # 512\n",
    "        self.up1 = StackDecoder( 32,  64,  32)  # 1024\n",
    "        \n",
    "        self.mask = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1, o = self.down1(x)\n",
    "        e2, o = self.down2(o)\n",
    "        e3, o = self.down3(o)\n",
    "        e4, o = self.down4(o)\n",
    "        e5, o = self.down5(o)\n",
    "#        e6, o = self.down6(o)\n",
    "\n",
    "        o = self.center(o)\n",
    "        \n",
    "#        o = self.up6(e6, o)\n",
    "        o = self.up5(e5, o)\n",
    "        o = self.up4(e4, o)\n",
    "        o = self.up3(e3, o)\n",
    "        o = self.up2(e2, o)\n",
    "        o = self.up1(e1, o)\n",
    "\n",
    "        o = self.mask(o)\n",
    "        o = F.upsample(o, size=(1280,1918), mode='bilinear')\n",
    "        return torch.squeeze(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:45.994977Z",
     "start_time": "2017-09-21T14:12:45.917472Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_score(probs, target, weight=None, use_mask=True, threshold=0.5):\n",
    "    if use_mask:\n",
    "        probs = (probs > threshold).float()\n",
    "    N     = target.size(0)\n",
    "    if weight is None:\n",
    "        w = Variable(torch.ones(target.size()).cuda()).view(N, -1)\n",
    "    else:\n",
    "        w = weight.view(N, -1)\n",
    "    w2    = w*w\n",
    "    m1    = probs.view(N, -1)\n",
    "    m2    = target.view(N, -1)\n",
    "    score = (2*(w2*m1*m2).sum(dim=1) + 1) / ((w2*m1).sum(dim=1)\n",
    "                                             + (w2*m2).sum(dim=1) + 1)\n",
    "    \n",
    "    return score.sum()/N\n",
    "\n",
    "\n",
    "def dice_loss(logits, target, weight=None):\n",
    "    probs = F.sigmoid(logits)\n",
    "    loss  = 1 - dice_score(probs, target, weight, use_mask=False)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def criterion(logits, target):\n",
    "    N,H,W = target.size()\n",
    "    a = F.avg_pool2d(target, kernel_size=41, stride=1, padding=20)\n",
    "    boundary = (a.ge(0.01) * a.le(0.99)).float()\n",
    "    weight = Variable(torch.ones(a.size()).cuda())\n",
    "    w0 = weight.sum()\n",
    "    weight = weight + 2*boundary\n",
    "    w1 = weight.sum()\n",
    "    weight = weight*w0/w1\n",
    "        \n",
    "    return (F.binary_cross_entropy_with_logits(logits, target, weight),\n",
    "            dice_loss(logits, target, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.055410Z",
     "start_time": "2017-09-21T14:12:45.996005Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_to_tensor(image):\n",
    "    image = image.transpose((2,0,1)).astype(np.float32)  # HWC -> CHW\n",
    "    tensor = torch.from_numpy(image)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def label_to_tensor(label, threshold=0.5):\n",
    "    label  = (label>threshold).astype(np.float32)\n",
    "    tensor = torch.from_numpy(label)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.149868Z",
     "start_time": "2017-09-21T14:12:46.059524Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CarDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path='', transform=[], mode='train'):\n",
    "        super(CarDataset, self).__init__()\n",
    "        self.img_names = sorted([x.split('/')[-1]\n",
    "                                 for x in glob.glob(image_path + '/*.jpg')])\n",
    "        self.img_path  = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.transform = transform\n",
    "        self.mode      = mode\n",
    "\n",
    "    def get_image(self, index):\n",
    "        name  = self.img_names[index]\n",
    "        file  = os.path.join(self.img_path, name)\n",
    "        img   = cv2.imread(file)\n",
    "        image = img / 255\n",
    "        return image, name\n",
    "    \n",
    "    def get_label(self, name):\n",
    "        name = name.replace('.jpg', '_mask.png')\n",
    "        file = os.path.join(self.mask_path, name)\n",
    "        mask = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        label = mask / 255\n",
    "        return label\n",
    "\n",
    "    def get_train_item(self, index):\n",
    "        image, name = self.get_image(index)\n",
    "        label = self.get_label(name)\n",
    "        image = image_to_tensor(image)\n",
    "        label = label_to_tensor(label)\n",
    "        return image, label\n",
    "\n",
    "    def get_test_item(self, index):\n",
    "        image, _ = self.get_image(index)\n",
    "        image = cv2.resize(image, (1024, 1024))\n",
    "        image = image_to_tensor(image)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            return self.get_train_item(index)\n",
    "        elif self.mode == 'test':\n",
    "            return self.get_test_item(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.213207Z",
     "start_time": "2017-09-21T14:12:46.151085Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "    num_grad_acc = 16 // args.batch_size\n",
    "    model.train()\n",
    "    train_bce_loss = 0\n",
    "    train_score = 0\n",
    "    init_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    for i, (inputt, target) in enumerate(train_loader, 1):\n",
    "        inputt = inputt.cuda()\n",
    "        target = target.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = model(inputt)\n",
    "        bce_loss, dice_loss = criterion(output, target)\n",
    "        loss = bce_loss + dice_loss\n",
    "        loss.backward()\n",
    "        if i % num_grad_acc == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_bce_loss += bce_loss.data[0] * inputt.size(0)\n",
    "        train_score += (1-dice_loss).data[0] * inputt.size(0)\n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train bce loss: {:.5f} | Train score: {:.5f}\"\n",
    "                  .format(time.time()-init_time, i,\n",
    "                          bce_loss.data[0], (1-dice_loss).data[0]))\n",
    "    return (train_bce_loss / len(train_loader.dataset),\n",
    "            train_score / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.283526Z",
     "start_time": "2017-09-21T14:12:46.214568Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model):\n",
    "    model.eval()\n",
    "    val_score = 0\n",
    "    for i, (inputt, target) in enumerate(val_loader, 1):\n",
    "        inputt = inputt.cuda()\n",
    "        target = target.cuda()\n",
    "        inputt = Variable(inputt, volatile=True)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = model(inputt)\n",
    "        score = dice_score(F.sigmoid(output), target)\n",
    "        val_score += score.data[0] * inputt.size(0)\n",
    "    return val_score / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.339920Z",
     "start_time": "2017-09-21T14:12:46.289034Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, score):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_{}_epoch{}_score{:.5f}.pth\"\n",
    "                              .format(args.seed, epoch, score))\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.391887Z",
     "start_time": "2017-09-21T14:12:46.341568Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(model, model_name):\n",
    "    model_path = os.path.join(args.intermediate_path, model_name)\n",
    "    assert os.path.isfile(model_path), 'Error: no model found!'\n",
    "    model_state = torch.load(model_path)\n",
    "    model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.459632Z",
     "start_time": "2017-09-21T14:12:46.396557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rle_encode(mask_image):\n",
    "    pixels = mask_image.flatten()\n",
    "    # We avoid issues with '1' at the start or end (at the corners of \n",
    "    # the original image) by setting those pixels to '0' explicitly.\n",
    "    # We do not expect these to be non-zero for an accurate mask, \n",
    "    # so this should not harm the score.\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.515148Z",
     "start_time": "2017-09-21T14:12:46.463986Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(test_loader, model, threshold=0.5):\n",
    "    model.eval()\n",
    "    init_time = time.time()\n",
    "    list_rle = []\n",
    "    for i, imgs in enumerate(test_loader, 1):\n",
    "        imgs = imgs.cuda()\n",
    "        imgs = Variable(imgs, volatile= True)\n",
    "        outputs = model(imgs)\n",
    "        for j in range(outputs.size(0)):\n",
    "            image = outputs[j, :, :].data.cpu().numpy()\n",
    "            cv2.imwrite(os.path.join(args.output_path, str{i}+'.jpg'), image)\n",
    "#        outputs = outputs > threshold\n",
    "#        for j in range(outputs.size(0)):\n",
    "#            rle = rle_encode(outputs[j, :, :].data.cpu().numpy())\n",
    "#            list_rle.append(' '.join(str(x) for x in rle))\n",
    "            \n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Image: {:6d} / {}\"\n",
    "                  .format(time.time()-init_time, i*args.batch_size,\n",
    "                          len(test_loader.dataset)))\n",
    "    return list_rle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.604157Z",
     "start_time": "2017-09-21T14:12:46.516776Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CarDataset(args.train_path, args.train_masks_path,\n",
    "                           transform=[], mode='train')\n",
    "train_loader  = DataLoader(train_dataset, args.batch_size, shuffle=True,\n",
    "                           num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:46.617147Z",
     "start_time": "2017-09-21T14:12:46.606065Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dataset = CarDataset(args.val_path, args.val_masks_path,\n",
    "                         transform=[], mode='train')\n",
    "val_loader  = DataLoader(val_dataset, args.batch_size, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T14:12:47.995831Z",
     "start_time": "2017-09-21T14:12:46.618471Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = UNet1024((3, 1024, 1024))\n",
    "model.cuda()\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9,\n",
    "#                      weight_decay=0.0005)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T00:00:33.549299Z",
     "start_time": "2017-09-21T14:12:47.997052Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.60300 | Train score: 0.47246\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.55125 | Train score: 0.47477\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.51689 | Train score: 0.50617\n",
      "   % Time:  241s | Batch:  320 | Train bce loss: 0.51320 | Train score: 0.51807\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.48286 | Train score: 0.55065\n",
      "   % Time:  362s | Batch:  480 | Train bce loss: 0.47676 | Train score: 0.57024\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.47748 | Train score: 0.55797\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.46643 | Train score: 0.57899\n",
      "   % Time:  543s | Batch:  720 | Train bce loss: 0.45160 | Train score: 0.58903\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.43973 | Train score: 0.60674\n",
      "   % Time:  664s | Batch:  880 | Train bce loss: 0.44309 | Train score: 0.60809\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.43541 | Train score: 0.59967\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.42506 | Train score: 0.61638\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.42131 | Train score: 0.61427\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.41486 | Train score: 0.62330\n",
      "   % Time:  966s | Batch: 1280 | Train bce loss: 0.40406 | Train score: 0.63317\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.40325 | Train score: 0.63090\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.38684 | Train score: 0.66569\n",
      "==========\n",
      "   % Time: 1137s | Epoch:    1 | Train bce loss: 0.47026 | Train score: 0.57396 | Val score: 0.99217\n",
      "==========\n",
      "=> EPOCH 2 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.39178 | Train score: 0.65726\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.38809 | Train score: 0.65958\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.38128 | Train score: 0.65331\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.37254 | Train score: 0.66653\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.37122 | Train score: 0.65924\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.36765 | Train score: 0.67978\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.36265 | Train score: 0.66819\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.35738 | Train score: 0.65723\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.34949 | Train score: 0.66504\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.34039 | Train score: 0.69568\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.32462 | Train score: 0.72150\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.35746 | Train score: 0.63683\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.32599 | Train score: 0.70757\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.31905 | Train score: 0.70395\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.31982 | Train score: 0.69218\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.31926 | Train score: 0.69598\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.31431 | Train score: 0.71194\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.32507 | Train score: 0.66805\n",
      "==========\n",
      "   % Time: 1138s | Epoch:    2 | Train bce loss: 0.35119 | Train score: 0.67234 | Val score: 0.99428\n",
      "==========\n",
      "=> EPOCH 3 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.30697 | Train score: 0.69311\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.28961 | Train score: 0.73607\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.29332 | Train score: 0.71954\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.28836 | Train score: 0.71251\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.27853 | Train score: 0.73685\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.28749 | Train score: 0.70668\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.27765 | Train score: 0.72447\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.28110 | Train score: 0.71935\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.27581 | Train score: 0.72423\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.26788 | Train score: 0.72763\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.25333 | Train score: 0.76700\n",
      "   % Time:  726s | Batch:  960 | Train bce loss: 0.26114 | Train score: 0.73568\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.25389 | Train score: 0.73841\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.25394 | Train score: 0.73565\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.24590 | Train score: 0.74974\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.25710 | Train score: 0.71150\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.23488 | Train score: 0.76743\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.25119 | Train score: 0.71771\n",
      "==========\n",
      "   % Time: 1138s | Epoch:    3 | Train bce loss: 0.26959 | Train score: 0.73149 | Val score: 0.99458\n",
      "==========\n",
      "=> EPOCH 4 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.22686 | Train score: 0.77792\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.22882 | Train score: 0.75606\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.23746 | Train score: 0.72562\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.24294 | Train score: 0.71268\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.20409 | Train score: 0.79854\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.20317 | Train score: 0.80687\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.20318 | Train score: 0.78530\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.19383 | Train score: 0.81317\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.20575 | Train score: 0.77940\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.18523 | Train score: 0.81386\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.18325 | Train score: 0.81772\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.20165 | Train score: 0.77799\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.18940 | Train score: 0.79678\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.19060 | Train score: 0.79133\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.17787 | Train score: 0.80954\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.16792 | Train score: 0.83039\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.17685 | Train score: 0.81257\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.17676 | Train score: 0.81075\n",
      "==========\n",
      "   % Time: 1138s | Epoch:    4 | Train bce loss: 0.20298 | Train score: 0.78273 | Val score: 0.99520\n",
      "==========\n",
      "=> EPOCH 5 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.16998 | Train score: 0.81496\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.17296 | Train score: 0.79667\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.16112 | Train score: 0.82972\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.14532 | Train score: 0.85181\n",
      "   % Time:  303s | Batch:  400 | Train bce loss: 0.15247 | Train score: 0.84068\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.16611 | Train score: 0.80126\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.16863 | Train score: 0.78671\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.16088 | Train score: 0.80634\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.14564 | Train score: 0.83596\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.14240 | Train score: 0.85483\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.15019 | Train score: 0.82014\n",
      "   % Time:  726s | Batch:  960 | Train bce loss: 0.14508 | Train score: 0.83521\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.12990 | Train score: 0.86751\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.14333 | Train score: 0.82441\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.13613 | Train score: 0.84464\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.13761 | Train score: 0.83895\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.13537 | Train score: 0.83711\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.13909 | Train score: 0.82505\n",
      "==========\n",
      "   % Time: 1138s | Epoch:    5 | Train bce loss: 0.15152 | Train score: 0.82638 | Val score: 0.99545\n",
      "==========\n",
      "=> EPOCH 6 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.12827 | Train score: 0.85577\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.12653 | Train score: 0.84977\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.11460 | Train score: 0.87384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.11056 | Train score: 0.88282\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.11887 | Train score: 0.85401\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.12685 | Train score: 0.83263\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.11856 | Train score: 0.86183\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.11698 | Train score: 0.85892\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.11776 | Train score: 0.85733\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.10624 | Train score: 0.88018\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.12040 | Train score: 0.83339\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.10470 | Train score: 0.87997\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.10102 | Train score: 0.88330\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.10642 | Train score: 0.86739\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.10208 | Train score: 0.87404\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.09744 | Train score: 0.88291\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.09801 | Train score: 0.88715\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.10089 | Train score: 0.86979\n",
      "==========\n",
      "   % Time: 1137s | Epoch:    6 | Train bce loss: 0.11282 | Train score: 0.86288 | Val score: 0.99586\n",
      "==========\n",
      "=> EPOCH 7 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.09608 | Train score: 0.87390\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.09935 | Train score: 0.86201\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.09128 | Train score: 0.89135\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.08931 | Train score: 0.88685\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.09172 | Train score: 0.88132\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.08503 | Train score: 0.89183\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.09359 | Train score: 0.87907\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.08020 | Train score: 0.90486\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.08600 | Train score: 0.89234\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.07695 | Train score: 0.90944\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.09039 | Train score: 0.86757\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.08194 | Train score: 0.88096\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.07988 | Train score: 0.89317\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.08290 | Train score: 0.88596\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.08117 | Train score: 0.89129\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.07026 | Train score: 0.90874\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.07882 | Train score: 0.88740\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.07609 | Train score: 0.89492\n",
      "==========\n",
      "   % Time: 1138s | Epoch:    7 | Train bce loss: 0.08405 | Train score: 0.89285 | Val score: 0.99611\n",
      "==========\n",
      "=> EPOCH 8 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.06963 | Train score: 0.91037\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.08106 | Train score: 0.87257\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.06291 | Train score: 0.91827\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.06569 | Train score: 0.91930\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.06915 | Train score: 0.90501\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.07579 | Train score: 0.88348\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.06503 | Train score: 0.90924\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.06446 | Train score: 0.92132\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.06062 | Train score: 0.92475\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.05802 | Train score: 0.93255\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.06346 | Train score: 0.91709\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.05635 | Train score: 0.93040\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.05818 | Train score: 0.92449\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.06244 | Train score: 0.91528\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.05946 | Train score: 0.92208\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.05604 | Train score: 0.93284\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.05944 | Train score: 0.92353\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.05984 | Train score: 0.92328\n",
      "==========\n",
      "   % Time: 1137s | Epoch:    8 | Train bce loss: 0.06500 | Train score: 0.91404 | Val score: 0.99544\n",
      "==========\n",
      "=> EPOCH 9 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.05141 | Train score: 0.94115\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.04767 | Train score: 0.94853\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.06103 | Train score: 0.91003\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.07654 | Train score: 0.92340\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.05113 | Train score: 0.93353\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.05935 | Train score: 0.91507\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.06537 | Train score: 0.91938\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.05474 | Train score: 0.91485\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.05263 | Train score: 0.92757\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.05070 | Train score: 0.93024\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.04608 | Train score: 0.94196\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.04335 | Train score: 0.93976\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.04856 | Train score: 0.92887\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.04606 | Train score: 0.94065\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.04330 | Train score: 0.94721\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.04624 | Train score: 0.93016\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.04817 | Train score: 0.92649\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.04775 | Train score: 0.92857\n",
      "==========\n",
      "   % Time: 1137s | Epoch:    9 | Train bce loss: 0.05018 | Train score: 0.93165 | Val score: 0.99623\n",
      "==========\n",
      "=> EPOCH 10 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.04195 | Train score: 0.94970\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.04409 | Train score: 0.93631\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.04197 | Train score: 0.94668\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.04081 | Train score: 0.94616\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.04364 | Train score: 0.93879\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.03882 | Train score: 0.94803\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.03784 | Train score: 0.95044\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.03848 | Train score: 0.94677\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.04254 | Train score: 0.93761\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.03841 | Train score: 0.94572\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.03781 | Train score: 0.95271\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.03476 | Train score: 0.96086\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.04017 | Train score: 0.93835\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.03458 | Train score: 0.94949\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.03700 | Train score: 0.95036\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.03510 | Train score: 0.94824\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.03658 | Train score: 0.94265\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.03688 | Train score: 0.94603\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   10 | Train bce loss: 0.04019 | Train score: 0.94407 | Val score: 0.99638\n",
      "==========\n",
      "=> EPOCH 11 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.03408 | Train score: 0.95592\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.04645 | Train score: 0.94937\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.03188 | Train score: 0.95810\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.03326 | Train score: 0.95406\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.03277 | Train score: 0.95373\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.03796 | Train score: 0.94390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.03538 | Train score: 0.94663\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.03629 | Train score: 0.94199\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.03559 | Train score: 0.94828\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.04023 | Train score: 0.94837\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.03694 | Train score: 0.95085\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.03339 | Train score: 0.95219\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.03441 | Train score: 0.95012\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.03240 | Train score: 0.95329\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.02813 | Train score: 0.95954\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.02829 | Train score: 0.95994\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.03160 | Train score: 0.95317\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.03331 | Train score: 0.96065\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   11 | Train bce loss: 0.03427 | Train score: 0.95183 | Val score: 0.99628\n",
      "==========\n",
      "=> EPOCH 12 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.02985 | Train score: 0.96186\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.02927 | Train score: 0.95901\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.02791 | Train score: 0.95770\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.03209 | Train score: 0.95645\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.03311 | Train score: 0.95739\n",
      "   % Time:  362s | Batch:  480 | Train bce loss: 0.02883 | Train score: 0.96215\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.03290 | Train score: 0.95524\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.03673 | Train score: 0.95275\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.03082 | Train score: 0.95691\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.02680 | Train score: 0.96063\n",
      "   % Time:  664s | Batch:  880 | Train bce loss: 0.02671 | Train score: 0.96185\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.02790 | Train score: 0.95772\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.02538 | Train score: 0.96629\n",
      "   % Time:  845s | Batch: 1120 | Train bce loss: 0.02896 | Train score: 0.95428\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.02540 | Train score: 0.95945\n",
      "   % Time:  966s | Batch: 1280 | Train bce loss: 0.02521 | Train score: 0.96817\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.02447 | Train score: 0.96368\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.02701 | Train score: 0.96074\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   12 | Train bce loss: 0.02916 | Train score: 0.95836 | Val score: 0.99650\n",
      "==========\n",
      "=> EPOCH 13 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.02519 | Train score: 0.96680\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.02293 | Train score: 0.97047\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.02366 | Train score: 0.96713\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.02395 | Train score: 0.96698\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.02142 | Train score: 0.97124\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.02643 | Train score: 0.96276\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.02338 | Train score: 0.96167\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.02412 | Train score: 0.96260\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.02442 | Train score: 0.96863\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.02606 | Train score: 0.96771\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.02439 | Train score: 0.95973\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.02417 | Train score: 0.96508\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.02442 | Train score: 0.96612\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.02191 | Train score: 0.96795\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.02156 | Train score: 0.96931\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.02181 | Train score: 0.97066\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.02412 | Train score: 0.96231\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.02653 | Train score: 0.96536\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   13 | Train bce loss: 0.02401 | Train score: 0.96517 | Val score: 0.99659\n",
      "==========\n",
      "=> EPOCH 14 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01943 | Train score: 0.97412\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.02044 | Train score: 0.97003\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.01958 | Train score: 0.96984\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.02136 | Train score: 0.97087\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.02448 | Train score: 0.95381\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.02100 | Train score: 0.96033\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.02482 | Train score: 0.96938\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.01983 | Train score: 0.96923\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.02003 | Train score: 0.97250\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01802 | Train score: 0.97559\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.01758 | Train score: 0.97360\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01752 | Train score: 0.97501\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.02107 | Train score: 0.96781\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.02062 | Train score: 0.97014\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.02227 | Train score: 0.96157\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.01766 | Train score: 0.97084\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01843 | Train score: 0.97142\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.01840 | Train score: 0.97373\n",
      "==========\n",
      "   % Time: 1138s | Epoch:   14 | Train bce loss: 0.02054 | Train score: 0.96981 | Val score: 0.99651\n",
      "==========\n",
      "=> EPOCH 15 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01842 | Train score: 0.97470\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01653 | Train score: 0.97495\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.02018 | Train score: 0.97244\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01850 | Train score: 0.97086\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01836 | Train score: 0.96899\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01749 | Train score: 0.97357\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01823 | Train score: 0.97605\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.01906 | Train score: 0.97077\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.01607 | Train score: 0.97581\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01548 | Train score: 0.97499\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.01698 | Train score: 0.97001\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01673 | Train score: 0.97441\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.01738 | Train score: 0.97087\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01905 | Train score: 0.97539\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01807 | Train score: 0.97391\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.01691 | Train score: 0.97423\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01505 | Train score: 0.97636\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.01747 | Train score: 0.97487\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   15 | Train bce loss: 0.01806 | Train score: 0.97321 | Val score: 0.99663\n",
      "==========\n",
      "=> EPOCH 16 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01985 | Train score: 0.97271\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01440 | Train score: 0.97905\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.01685 | Train score: 0.97079\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01864 | Train score: 0.97223\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01420 | Train score: 0.97985\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01563 | Train score: 0.97415\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01681 | Train score: 0.97575\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.01611 | Train score: 0.97450\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.01471 | Train score: 0.97957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01658 | Train score: 0.97784\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.01490 | Train score: 0.97719\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01664 | Train score: 0.97701\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.01896 | Train score: 0.97610\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01607 | Train score: 0.97648\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01696 | Train score: 0.97710\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.01642 | Train score: 0.97674\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01676 | Train score: 0.98042\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.01646 | Train score: 0.97408\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   16 | Train bce loss: 0.01606 | Train score: 0.97600 | Val score: 0.99664\n",
      "==========\n",
      "=> EPOCH 17 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01564 | Train score: 0.97476\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01353 | Train score: 0.97980\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.01604 | Train score: 0.97504\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01562 | Train score: 0.97874\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01407 | Train score: 0.97961\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01514 | Train score: 0.97607\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01590 | Train score: 0.97607\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.01572 | Train score: 0.97959\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.01322 | Train score: 0.98035\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01290 | Train score: 0.97774\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.01802 | Train score: 0.97491\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01418 | Train score: 0.97766\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.01400 | Train score: 0.98170\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01396 | Train score: 0.97760\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01414 | Train score: 0.98062\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.01253 | Train score: 0.97882\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01336 | Train score: 0.98189\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.01241 | Train score: 0.97956\n",
      "==========\n",
      "   % Time: 1138s | Epoch:   17 | Train bce loss: 0.01461 | Train score: 0.97799 | Val score: 0.99648\n",
      "==========\n",
      "=> EPOCH 18 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01546 | Train score: 0.97868\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01380 | Train score: 0.97841\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.01349 | Train score: 0.98053\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01313 | Train score: 0.98116\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01388 | Train score: 0.97792\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01252 | Train score: 0.97981\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01144 | Train score: 0.98326\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.01462 | Train score: 0.97878\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.01143 | Train score: 0.98219\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01303 | Train score: 0.97904\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.01168 | Train score: 0.98096\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01427 | Train score: 0.98053\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.01356 | Train score: 0.98004\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01397 | Train score: 0.97392\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01413 | Train score: 0.97788\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.01247 | Train score: 0.98084\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01295 | Train score: 0.97924\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.01261 | Train score: 0.98064\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   18 | Train bce loss: 0.01343 | Train score: 0.97966 | Val score: 0.99670\n",
      "==========\n",
      "=> EPOCH 19 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01534 | Train score: 0.97305\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.03022 | Train score: 0.96546\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.01829 | Train score: 0.97150\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01400 | Train score: 0.97769\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.02270 | Train score: 0.97545\n",
      "   % Time:  362s | Batch:  480 | Train bce loss: 0.01564 | Train score: 0.97826\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01273 | Train score: 0.98048\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.01220 | Train score: 0.98261\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.01064 | Train score: 0.98209\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01105 | Train score: 0.98315\n",
      "   % Time:  664s | Batch:  880 | Train bce loss: 0.01329 | Train score: 0.97987\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01589 | Train score: 0.97885\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.01266 | Train score: 0.97652\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01047 | Train score: 0.98470\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01390 | Train score: 0.98277\n",
      "   % Time:  966s | Batch: 1280 | Train bce loss: 0.01039 | Train score: 0.98339\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01261 | Train score: 0.98288\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.01124 | Train score: 0.98132\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   19 | Train bce loss: 0.01519 | Train score: 0.97793 | Val score: 0.99671\n",
      "==========\n",
      "=> EPOCH 20 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01456 | Train score: 0.97763\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01276 | Train score: 0.98261\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.01074 | Train score: 0.98250\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01019 | Train score: 0.98273\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01203 | Train score: 0.97803\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01238 | Train score: 0.98089\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01051 | Train score: 0.98173\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.01102 | Train score: 0.98164\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.01456 | Train score: 0.98144\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01125 | Train score: 0.98254\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00988 | Train score: 0.98388\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01032 | Train score: 0.98200\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.01096 | Train score: 0.98089\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01154 | Train score: 0.98283\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01341 | Train score: 0.97998\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.01151 | Train score: 0.98394\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01100 | Train score: 0.98310\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.01187 | Train score: 0.98314\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   20 | Train bce loss: 0.01187 | Train score: 0.98184 | Val score: 0.99678\n",
      "==========\n",
      "=> EPOCH 21 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01057 | Train score: 0.98235\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.00938 | Train score: 0.98559\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.01127 | Train score: 0.98392\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01136 | Train score: 0.98390\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01090 | Train score: 0.98362\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01093 | Train score: 0.98236\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01063 | Train score: 0.98250\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.01329 | Train score: 0.98208\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00891 | Train score: 0.98719\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.00985 | Train score: 0.98334\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.01019 | Train score: 0.98335\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01059 | Train score: 0.98357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.00972 | Train score: 0.98484\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01176 | Train score: 0.98163\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.00932 | Train score: 0.98535\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.01078 | Train score: 0.98283\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.00994 | Train score: 0.98365\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.00934 | Train score: 0.98548\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   21 | Train bce loss: 0.01088 | Train score: 0.98329 | Val score: 0.99680\n",
      "==========\n",
      "=> EPOCH 22 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00785 | Train score: 0.98659\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01032 | Train score: 0.98430\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.01200 | Train score: 0.98294\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01000 | Train score: 0.97989\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01021 | Train score: 0.98599\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00979 | Train score: 0.98406\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00859 | Train score: 0.98447\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.01004 | Train score: 0.98241\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00910 | Train score: 0.98327\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01249 | Train score: 0.98288\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00862 | Train score: 0.98614\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.00821 | Train score: 0.98479\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.01004 | Train score: 0.98467\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.01399 | Train score: 0.97872\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01669 | Train score: 0.97740\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.02070 | Train score: 0.97295\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.03439 | Train score: 0.96877\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.01548 | Train score: 0.98029\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   22 | Train bce loss: 0.01232 | Train score: 0.98217 | Val score: 0.99612\n",
      "==========\n",
      "=> EPOCH 23 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01113 | Train score: 0.98269\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01851 | Train score: 0.97904\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.01337 | Train score: 0.98258\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01245 | Train score: 0.98055\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.01218 | Train score: 0.97946\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01260 | Train score: 0.98380\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01291 | Train score: 0.98382\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.00906 | Train score: 0.98353\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00934 | Train score: 0.98432\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.00723 | Train score: 0.98704\n",
      "   % Time:  664s | Batch:  880 | Train bce loss: 0.00950 | Train score: 0.98576\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.00959 | Train score: 0.98384\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.01140 | Train score: 0.98473\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00897 | Train score: 0.98241\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01311 | Train score: 0.98294\n",
      "   % Time:  966s | Batch: 1280 | Train bce loss: 0.00937 | Train score: 0.98335\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01295 | Train score: 0.98425\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.00804 | Train score: 0.98726\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   23 | Train bce loss: 0.01076 | Train score: 0.98347 | Val score: 0.99675\n",
      "==========\n",
      "=> EPOCH 24 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.01071 | Train score: 0.98562\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.00816 | Train score: 0.98615\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.00712 | Train score: 0.98640\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.01125 | Train score: 0.98300\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.00875 | Train score: 0.98492\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00918 | Train score: 0.98415\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00973 | Train score: 0.98454\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.01048 | Train score: 0.98596\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.01155 | Train score: 0.98565\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.00886 | Train score: 0.98538\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00721 | Train score: 0.98565\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.00842 | Train score: 0.98420\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.01037 | Train score: 0.98541\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00845 | Train score: 0.98446\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.01045 | Train score: 0.98453\n",
      "   % Time:  966s | Batch: 1280 | Train bce loss: 0.00817 | Train score: 0.98582\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.01317 | Train score: 0.98268\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.01014 | Train score: 0.98383\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   24 | Train bce loss: 0.00959 | Train score: 0.98510 | Val score: 0.99669\n",
      "==========\n",
      "=> EPOCH 25 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00819 | Train score: 0.98753\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.00790 | Train score: 0.98524\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.01063 | Train score: 0.98369\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.00679 | Train score: 0.98717\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.00846 | Train score: 0.98581\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.01049 | Train score: 0.98536\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.01042 | Train score: 0.98378\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.00841 | Train score: 0.98465\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00867 | Train score: 0.98631\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.00766 | Train score: 0.98589\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00950 | Train score: 0.98480\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.01051 | Train score: 0.98385\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.00854 | Train score: 0.98588\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00934 | Train score: 0.98443\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.00859 | Train score: 0.98773\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.00877 | Train score: 0.98728\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.00948 | Train score: 0.98339\n",
      "   % Time: 1087s | Batch: 1440 | Train bce loss: 0.00887 | Train score: 0.98500\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   25 | Train bce loss: 0.00913 | Train score: 0.98576 | Val score: 0.99673\n",
      "==========\n",
      "=> EPOCH 26 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00909 | Train score: 0.98594\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01330 | Train score: 0.98215\n",
      "   % Time:  181s | Batch:  240 | Train bce loss: 0.00876 | Train score: 0.98513\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.00772 | Train score: 0.98769\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.00809 | Train score: 0.98629\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00706 | Train score: 0.98629\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00922 | Train score: 0.98630\n",
      "   % Time:  483s | Batch:  640 | Train bce loss: 0.00753 | Train score: 0.98898\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00692 | Train score: 0.98932\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.01118 | Train score: 0.98575\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.01174 | Train score: 0.98386\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.00815 | Train score: 0.98620\n",
      "   % Time:  785s | Batch: 1040 | Train bce loss: 0.00743 | Train score: 0.98765\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00843 | Train score: 0.98624\n",
      "   % Time:  906s | Batch: 1200 | Train bce loss: 0.00811 | Train score: 0.98438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.00763 | Train score: 0.98710\n",
      "   % Time: 1027s | Batch: 1360 | Train bce loss: 0.00972 | Train score: 0.98526\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.01167 | Train score: 0.98445\n",
      "==========\n",
      "   % Time: 1137s | Epoch:   26 | Train bce loss: 0.00866 | Train score: 0.98643 | Val score: 0.99682\n",
      "==========\n",
      "=> EPOCH 27 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00940 | Train score: 0.98482\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.00614 | Train score: 0.98886\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.00768 | Train score: 0.98701\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.00714 | Train score: 0.98744\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.00804 | Train score: 0.98653\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00789 | Train score: 0.98632\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00640 | Train score: 0.98624\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.00879 | Train score: 0.98704\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00747 | Train score: 0.98730\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.00736 | Train score: 0.98823\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00810 | Train score: 0.98565\n",
      "   % Time:  726s | Batch:  960 | Train bce loss: 0.00850 | Train score: 0.98426\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.00879 | Train score: 0.98771\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00941 | Train score: 0.98595\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.00607 | Train score: 0.98922\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.00582 | Train score: 0.98885\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.00773 | Train score: 0.98738\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.00862 | Train score: 0.98589\n",
      "==========\n",
      "   % Time: 1138s | Epoch:   27 | Train bce loss: 0.00841 | Train score: 0.98682 | Val score: 0.99676\n",
      "==========\n",
      "=> EPOCH 28 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00731 | Train score: 0.98750\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.01040 | Train score: 0.98623\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.00593 | Train score: 0.98885\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.00794 | Train score: 0.98763\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.00880 | Train score: 0.98598\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00579 | Train score: 0.98915\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00804 | Train score: 0.98773\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.00917 | Train score: 0.98627\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00893 | Train score: 0.98708\n",
      "   % Time:  604s | Batch:  800 | Train bce loss: 0.00897 | Train score: 0.98752\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00990 | Train score: 0.98755\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.00576 | Train score: 0.98894\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.00582 | Train score: 0.98840\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00953 | Train score: 0.98528\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.00644 | Train score: 0.98835\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.00789 | Train score: 0.98639\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.00705 | Train score: 0.98812\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.00847 | Train score: 0.98639\n",
      "==========\n",
      "   % Time: 1138s | Epoch:   28 | Train bce loss: 0.00800 | Train score: 0.98739 | Val score: 0.99683\n",
      "==========\n",
      "=> EPOCH 29 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00765 | Train score: 0.98691\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.00814 | Train score: 0.98737\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.00833 | Train score: 0.98725\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.00583 | Train score: 0.98870\n",
      "   % Time:  302s | Batch:  400 | Train bce loss: 0.00763 | Train score: 0.98799\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00750 | Train score: 0.98832\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00782 | Train score: 0.98801\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.00740 | Train score: 0.98804\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00569 | Train score: 0.98894\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.00664 | Train score: 0.98940\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00662 | Train score: 0.98869\n",
      "   % Time:  725s | Batch:  960 | Train bce loss: 0.00642 | Train score: 0.98989\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.00869 | Train score: 0.98785\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00920 | Train score: 0.98735\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.00722 | Train score: 0.98715\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.00776 | Train score: 0.98823\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.00702 | Train score: 0.98737\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.00585 | Train score: 0.98962\n",
      "==========\n",
      "   % Time: 1138s | Epoch:   29 | Train bce loss: 0.00767 | Train score: 0.98788 | Val score: 0.99678\n",
      "==========\n",
      "=> EPOCH 30 with lr [0.0001]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00851 | Train score: 0.98713\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.00782 | Train score: 0.98750\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.00860 | Train score: 0.98836\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.00637 | Train score: 0.98952\n",
      "   % Time:  303s | Batch:  400 | Train bce loss: 0.00570 | Train score: 0.98840\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00804 | Train score: 0.98676\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00619 | Train score: 0.98888\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.01060 | Train score: 0.98561\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00615 | Train score: 0.98871\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.00584 | Train score: 0.98977\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00788 | Train score: 0.98932\n",
      "   % Time:  726s | Batch:  960 | Train bce loss: 0.00940 | Train score: 0.98693\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.00758 | Train score: 0.98928\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00839 | Train score: 0.98816\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.00671 | Train score: 0.98800\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.00743 | Train score: 0.98836\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.00815 | Train score: 0.98817\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.00675 | Train score: 0.98908\n",
      "==========\n",
      "   % Time: 1138s | Epoch:   30 | Train bce loss: 0.00745 | Train score: 0.98819 | Val score: 0.99680\n",
      "==========\n",
      "=> EPOCH 31 with lr [1e-05]\n",
      "   % Time:   61s | Batch:   80 | Train bce loss: 0.00699 | Train score: 0.98981\n",
      "   % Time:  121s | Batch:  160 | Train bce loss: 0.00576 | Train score: 0.99008\n",
      "   % Time:  182s | Batch:  240 | Train bce loss: 0.01119 | Train score: 0.98782\n",
      "   % Time:  242s | Batch:  320 | Train bce loss: 0.00858 | Train score: 0.98813\n",
      "   % Time:  303s | Batch:  400 | Train bce loss: 0.00710 | Train score: 0.98959\n",
      "   % Time:  363s | Batch:  480 | Train bce loss: 0.00729 | Train score: 0.98904\n",
      "   % Time:  423s | Batch:  560 | Train bce loss: 0.00739 | Train score: 0.98903\n",
      "   % Time:  484s | Batch:  640 | Train bce loss: 0.00751 | Train score: 0.98906\n",
      "   % Time:  544s | Batch:  720 | Train bce loss: 0.00686 | Train score: 0.98749\n",
      "   % Time:  605s | Batch:  800 | Train bce loss: 0.00702 | Train score: 0.98935\n",
      "   % Time:  665s | Batch:  880 | Train bce loss: 0.00717 | Train score: 0.98980\n",
      "   % Time:  726s | Batch:  960 | Train bce loss: 0.00895 | Train score: 0.98756\n",
      "   % Time:  786s | Batch: 1040 | Train bce loss: 0.00624 | Train score: 0.98899\n",
      "   % Time:  846s | Batch: 1120 | Train bce loss: 0.00702 | Train score: 0.98989\n",
      "   % Time:  907s | Batch: 1200 | Train bce loss: 0.00573 | Train score: 0.98952\n",
      "   % Time:  967s | Batch: 1280 | Train bce loss: 0.00557 | Train score: 0.99050\n",
      "   % Time: 1028s | Batch: 1360 | Train bce loss: 0.00595 | Train score: 0.99077\n",
      "   % Time: 1088s | Batch: 1440 | Train bce loss: 0.00676 | Train score: 0.99016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "   % Time: 1138s | Epoch:   31 | Train bce loss: 0.00662 | Train score: 0.98938 | Val score: 0.99688\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, 32):\n",
    "        scheduler.step()\n",
    "        print(\"=> EPOCH {} with lr {}\".format(epoch, scheduler.get_lr()))\n",
    "        init_time = time.time()\n",
    "        train_bce_loss, train_score = train(train_loader, model, optimizer)\n",
    "        val_score = validate(val_loader, model)\n",
    "        print(\"=\"*10)\n",
    "        print(\"   % Time: {:4.0f}s | Epoch: {:4} | Train bce loss: {:.5f} \"\n",
    "              \"| Train score: {:.5f} | Val score: {:.5f}\"\n",
    "              .format(time.time()-init_time, epoch, train_bce_loss,\n",
    "                      train_score, val_score))\n",
    "        print(\"=\"*10)\n",
    "        save_model(model, epoch, val_score)\n",
    "else:\n",
    "    load_model(model, args.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T00:00:33.555788Z",
     "start_time": "2017-09-22T00:00:33.550576Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.test:\n",
    "    test_dataset = CarDataset(args.test_path, transform=[], mode='test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)\n",
    "    list_rle = test(test_loader, model)\n",
    "    \n",
    "#    df = pd.DataFrame({\"img\": test_dataset.img_names, \"rle_mask\": list_rle})\n",
    "#    submiss_path = os.path.join(args.intermediate_path, \"submission.csv\")\n",
    "#    df.to_csv(submiss_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydata)",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
